import json, csv, os, logging, glob
from datetime import datetime
from multiprocessing import Process, Manager
import pandas as pd
from sqlalchemy import create_engine

logging.basicConfig(
    level=logging.INFO,
    filename='/app/vietstock/crawled_data/crawl.log',
    filemode='a',  # Ch·∫ø ƒë·ªô append
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

class SeleniumPipeline:
    """ Pipeline ƒë·ªÉ x·ª≠ l√Ω v√† l∆∞u d·ªØ li·ªáu t·ª´ Selenium """


    def __init__(self, output_folder= f"vietstock/crawled_data"):
        self.output_folder = output_folder
        os.makedirs(self.output_folder, exist_ok=True)  # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        self.data = {}  # Dictionary ch·ª©a d·ªØ li·ªáu theo t·ª´ng b·∫£ng
        self._seen_keys = {}  # NEW: gi·ªØ c√°c key duy nh·∫•t ƒë√£ g·∫∑p
    
    def get_unique_key(self, item, source):
        # Tu·ª≥ theo b·∫£ng m√† t·∫°o key kh√°c nhau
        if source == "HNX_DSCK":
            return item.get("CK_id", "").strip()
        elif source == "HNX_KQGD":
            return f"{item.get('CK_id', '').strip()}|{item.get('date', '').strip()}"
        elif source  in ("DSCK_HNX", "DSCK_HSX", "DSCK_UpCom"):
            return item.get("M√£", "").strip()
        elif source == "HNX_KQGD_CAFEF":
            return f"{item.get('CK_id', '').strip()}|{item.get('ngay', '').strip()}"
        elif source in (f"vietstock_company_{item.get('exchange')},"): #vietstock company
            return item.get('CK_id', '').strip()
        elif source in (f"vietstock_price_{item.get('exchange')}_{item.get('ngay')}"): #vietstock price
            return f"{item.get('maCK', '').strip()}|{item.get('ngay', '').strip()}"
        elif source == "Vietstock_symbol_News":#vietstock news symbol
            return f"{item.get('title', '').strip()}|{item.get('url', '').strip()}"
        elif source == "Vietstock_News_Latest": #vietstock News_Latest
            return f"{item.get('title', '').strip()}|{item.get('url', '').strip()}"
        return json.dumps(item, sort_keys=True)  # fallback


    def process_item(self, item, source):
        if source not in self.data:
            self.data[source] = []
            self._seen_keys[source] = set()  # ƒê·∫£m b·∫£o kh·ªüi t·∫°o lu√¥n _seen_keys cho source n√†y
        
        unique_key = self.get_unique_key(item, source)
        
            # B·ªè qua n·∫øu key kh√¥ng h·ª£p l·ªá
        if not unique_key or unique_key == "|":
            print(f"[‚ö†Ô∏è B·ªè qua] D·ªØ li·ªáu thi·∫øu CK_id ho·∫∑c date_value ‚Üí {item}")
            logging.warning(f"[‚ö†Ô∏è B·ªè qua] D·ªØ li·ªáu thi·∫øu CK_id ho·∫∑c date_value ‚Üí {item}")
            return

        if unique_key in self._seen_keys[source]:
            print(f"[‚ö†Ô∏è Duplicate] {unique_key} ƒë√£ t·ªìn t·∫°i, b·ªè qua.")
            logging.warning(f"[‚ö†Ô∏è Duplicate] {unique_key} ƒë√£ t·ªìn t·∫°i, b·ªè qua.")
            return

        self._seen_keys[source].add(unique_key)
        processed_item = getattr(self, f"process_{source}", self.default_process)(item)
        self.data[source].append(processed_item)
        print(f"‚úîÔ∏è ƒê√£ x·ª≠ l√Ω: {unique_key}")
        logging.info(f"‚úîÔ∏è ƒê√£ x·ª≠ l√Ω: {unique_key}")

    def process_HNX_DSCK(self, item):
        """ X·ª≠ l√Ω b·∫£ng HNX_DSCK """
        return {
            "stt": item.get("stt", "").strip(),
            "M√£ ch·ª©ng kho√°n": item.get("CK_id", "").strip(),
            "T√™n t·ªï ch·ª©c": item.get("organization", "").strip(),
            "Ng√†nh": item.get("branch", "").strip(),
            "Ng√†y giao d·ªãch ƒë·∫ßu ti√™n": item.get("start_trading_date", "").strip(),
            "Kh·ªëi l∆∞·ª£ng ni√™m y·∫øt": item.get("listed_volume", "").replace(",", "").strip(),
            "T·ªïng gi√° tr·ªã ni√™m y·∫øt": item.get("total_listing_value", "").replace(",", "").strip()
        }
    
    def process_HNX_KQGD(self, item):
        return {
            "S·ªë tt": (item.get("STT") or "").strip(),
            "Ng√†y": (item.get("date") or "").strip(),
            "M√£ CK": (item.get("CK_id") or "").strip(),
            "Gi√° tham chi·∫øu": (item.get("gia_tham_chieu") or "").strip(),
            "Gi√° tr·∫ßn": (item.get("gia_tran") or "").strip(),
            "Gi√° s√†n": (item.get("gia_san") or "").strip(),
            "Gi√° m·ªü c·ª≠a ": (item.get("gia_mo_cua") or "").replace(",", "").strip(),
            "Gi√° ƒë√≥ng c·ª≠a ": (item.get("gia_dong_cua") or "").replace(",", "").strip(),
            "Gi√° cao nh·∫•t": (item.get("gia_cao_nhat") or "").replace(",", "").strip(),
            "Gi√° Th·∫•p Nh·∫•t": (item.get("gia_thap_nhat") or "").replace(",", "").strip(),
            "Thay ƒë·ªïi (ƒêi·ªÉm)": (item.get("thay_doi_diem") or "").replace(",", "").strip(),
            "Thay ƒë·ªïi (%)": (item.get("thay_doi_phan_tram") or "").replace(",", "").strip(),
        }

    def process_HNX_KQGD_CAFEF(self, item):
        return {
            "Ng√†y": (item.get("ngay") or "").strip(),
            "ƒê√≥ng c·ª≠a": (item.get("dong_cua") or "").strip(),
            "ƒêi·ªÅu ch·ªânh": (item.get("dieu_chinh") or "").strip(),
            "Thay ƒë·ªïi": (item.get("thay_doi") or "").strip(),
            "Kh·ªëi l∆∞·ª£ng GDKL": (item.get("khoi_luong_GDKL") or "").strip(),
            "Gi√° tr·ªã GDKL ": (item.get("gia_tri_GDKL") or "").replace(",", "").strip(),
            "Kh·ªëi l∆∞·ª£ng GDTT ": (item.get("khoi_luong_GDTT") or "").replace(",", "").strip(),
            "Gi√° tr·ªã GDTT": (item.get("gia_tri_GDTT") or "").replace(",", "").strip(),
            "M·ªü c·ª≠a": (item.get("mo_cua") or "").replace(",", "").strip(),
            "Cao nh·∫•t": (item.get("cao_nhat") or "").replace(",", "").strip(),
            "Th·∫•p nh·∫•t": (item.get("thap_nhat") or "").replace(",", "").strip(),
        }
    

    def process_Vietstock_Symbol_News(self, item):
    # Chu·∫©n h√≥a ng√†y
        try:
            date_obj = datetime.strptime(item.get("date", ""), "%d/%m/%Y")
            date_str = date_obj.strftime("%Y-%m-%d")
        except:
            date_str = item.get("date", "").strip()

        return {
            "URL": item.get("url", "").strip(),
            "M√£ CK": item.get("symbol", "").strip(),
            "Ti√™u ƒë·ªÅ": item.get("title", "").strip(),
            "N·ªôi dung": (item.get("content") or "").strip(),
            "Ng√†y": date_str,
        }
    
    def process_Vietstock_News_latest(self, item):
        try:
            date_obj = datetime.strptime(item.get("date", ""), "%d/%m/%Y")
            date_str = date_obj.strftime("%Y-%m-%d")
        except:
            date_str = item.get("date", "").strip()

        return {
            "date": date_str,
            "title": item.get("title", "").strip(),
            "content": (item.get("content") or "").strip(),
            "author" : (item.get("author") or "").strip(),
            "publish_time" : (item.get("publish_time") or "").strip(),
            "URL": item.get("url", "").strip(),
        }

    def default_process(self, item):
        """ X·ª≠ l√Ω m·∫∑c ƒë·ªãnh n·∫øu ch∆∞a c√≥ h√†m ri√™ng """
        return item

    def save_data(self, temp=False):

        # L∆∞u t·ª´ng b·∫£ng d·ªØ li·ªáu ra file CSV.
        # N·∫øu temp=True ‚Üí ghi file t·∫°m (_tmp.csv) 
        # N·∫øu temp=False ‚Üí ghi file ch√≠nh (.csv)
        
        for source, items in self.data.items():
            if not items:
                continue

            safe_source = source.replace("/", "-")
            suffix = "_tmp.csv" if temp else ".csv"
            file_path = os.path.join(self.output_folder, f"{safe_source}{suffix}")

            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            headers = list(items[0].keys())

            with open(file_path, "w", encoding="utf-8", newline="") as f:
                writer = csv.DictWriter(f, fieldnames=headers, quoting=csv.QUOTE_ALL)
                writer.writeheader()
                writer.writerows(items)

            print(f"üíæ ƒê√£ l∆∞u file { 't·∫°m' if temp else 'ch√≠nh' }: {file_path} ({len(items)} d√≤ng)")
            logging.info(f"üíæ ƒê√£ l∆∞u file { 't·∫°m' if temp else 'ch√≠nh' }: {file_path} ({len(items)} d√≤ng)")


"""
# h√†m g·ªôp file 
def merge_csv_files(pattern, output_file, remove_temp=True):
    all_files = glob.glob(pattern)
    if not all_files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file t·∫°m ƒë·ªÉ g·ªôp.")
        return

    df_list = [pd.read_csv(f) for f in all_files]
    merged_df = pd.concat(df_list, ignore_index=True)
    merged_df.drop_duplicates(inplace=True)

    merged_df.to_csv(output_file, index=False, encoding="utf-8")
    print(f"‚úÖ ƒê√£ g·ªôp {len(all_files)} file th√†nh {output_file} ({len(merged_df)} d√≤ng)")

    if remove_temp:
        for f in all_files:
            os.remove(f)
            print(f"üóëÔ∏è ƒê√£ xo√° file t·∫°m: {f}")
"""

import glob
import pandas as pd
import os

def merge_csv_files(pattern, output_file):
    new_files = glob.glob(pattern)
    if not new_files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file m·ªõi.")
        return

    # ƒê·ªçc t·∫•t c·∫£ file m·ªõi crawl
    new_df = pd.concat((pd.read_csv(f) for f in new_files), ignore_index=True)

    # N·∫øu file output ƒë√£ c√≥ ‚ûî g·ªôp th√™m
    if os.path.exists(output_file):
        old_df = pd.read_csv(output_file)
        merged_df = pd.concat([old_df, new_df], ignore_index=True)
        merged_df.drop_duplicates(subset=["CK_id"] , keep="first", inplace=True)
    else:
        merged_df = new_df

    merged_df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"‚úÖ Merge th√†nh c√¥ng {len(new_files)} file v√†o {output_file}")

    # X√≥a file t·∫°m sau khi merge
    for f in new_files:
        try:
            os.remove(f)
            print(f"üóëÔ∏è ƒê√£ x√≥a file t·∫°m: {f}")
        except Exception as e:
            print(f"‚ùå L·ªói khi x√≥a {f}: {e}")



from sqlalchemy import create_engine
import pandas as pd
import logging

from sqlalchemy import create_engine, text  # Th√™m import text

"""
def save_csv_to_postgres(csv_file, db_url, table_name, if_exists="replace"):
    try:
        df = pd.read_csv(csv_file)
        logging.info(f"ƒê·ªçc {len(df)} d√≤ng t·ª´ CSV th√†nh c√¥ng.")
    except Exception as e:
        logging.error(f"Kh√¥ng th·ªÉ ƒë·ªçc CSV file {csv_file}: {str(e)}")
        return
    
    logging.info(f"C√°c c·ªôt trong DataFrame: {df.columns.tolist()}")
    logging.info(f"D√≤ng ƒë·∫ßu ti√™n: {df.iloc[0].to_dict()}")

    df = df.rename(columns={
        'CK_id': 'symbol',
        'company': 'company_name',
        'branch': 'industry',
        'exchange': 'exchange',
        'listed_volume': 'listed_volume'
    })
    df = df.drop(columns=['stt'], errors='ignore')
    df['listed_volume'] = df['listed_volume'].astype(str).str.replace('"', '').str.replace(',', '').astype(int)
    df = df.drop_duplicates(subset=['symbol', 'exchange'])
    
    logging.info(f"S·ªë d√≤ng sau khi l√†m s·∫°ch v√† lo·∫°i tr√πng: {len(df)}")
    logging.info(f"C√°c c·ªôt sau khi √°nh x·∫°: {df.columns.tolist()}")

    try:
        engine = create_engine(db_url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)
        logging.info("ƒê√£ k·∫øt n·ªëi ƒë·∫øn DB")
    except Exception as e:
        logging.error(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi c∆° s·ªü d·ªØ li·ªáu PostgreSQL: {str(e)}")
        return

    if if_exists == "replace":
        try:
            with engine.connect() as conn:
                # S·ª≠ d·ª•ng text() ƒë·ªÉ th·ª±c thi l·ªánh TRUNCATE
                conn.execute(text(f"TRUNCATE TABLE {table_name}"))
                conn.commit()  # ƒê·∫£m b·∫£o commit l·ªánh TRUNCATE
                logging.info("ƒê√£ x√≥a d·ªØ li·ªáu c≈© trong b·∫£ng.")
        except Exception as e:
            logging.error(f"Kh√¥ng th·ªÉ TRUNCATE b·∫£ng {table_name}: {str(e)}")
            return

    try:
        df.to_sql(
            table_name,
            engine,
            index=False,
            if_exists="append",
            method="multi",
            chunksize=1000
        )
        logging.info(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu {len(df)} d√≤ng v√†o b·∫£ng {table_name}.")
    except Exception as e:
        logging.error(f"Kh√¥ng th·ªÉ l∆∞u d·ªØ li·ªáu v√†o b·∫£ng {table_name}: {str(e)}")
        raise
"""

TABLE_MAPPINGS = {
    "crawler_company": {
        "column_mapping": {
            'CK_id': 'symbol',
            'company': 'company_name',
            'branch': 'industry',
            'exchange': 'exchange',
            'listed_volume': 'listed_volume'
        },
        "drop_columns": ['stt'],
        "unique_columns": ['symbol', 'exchange'],
        "data_transforms": {
            'listed_volume': lambda x: int(str(x).replace('"', '').replace(',', ''))
        },
        "dependent_tables": None
    },
    "crawler_price": {
        "column_mapping": {
            'ngay' : 'date',
            'exchange': 'exchange',
            'maCK': 'stock',
            'tham_chieu' : 'basic',
            'mo_cua' : 'open',
            'dong_cua' : 'close',
            'cao_nhat' : 'high',
            'thap_nhat' : 'low',
            'trung_binh' : 'average',
            'thay_doi_tang_giam' : 'change_abs',
            'thay_doi_phan_tram' : 'change_percent',
            'kl_gdkl' : 'order_matching_vol',
            'gt_gdkl' : 'order_matching_val',
            'kl_gdtt' : 'put_through_vol',
            'gt_gdtt' : 'put_through_val',
            'kl_tgd' : 'total_transaction_vol',
            'gt_tgd' : 'total_transaction_val',
            'von_hoa' : 'market_cap'
        },
        "drop_columns": ['stt'],
        "unique_columns": ['stock', 'date'],
        "data_transforms": {
            'date': lambda x: pd.to_datetime(x, format= '%d/%m/%Y', errors='coerce').date() if pd.notnull(x) else None,
            'basic': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'open': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'close': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'high': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'low': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'average': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-' else None,
            'change_abs': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'change_percent': lambda x: float(str(x).replace('%', '').replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'order_matching_vol': lambda x: int(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'order_matching_val': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'put_through_vol': lambda x: int(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'put_through_val': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'total_transaction_vol': lambda x: int(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'total_transaction_val': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
            'market_cap': lambda x: float(str(x).replace(',', '').replace('"', '')) if pd.notnull(x) and str(x).replace(',', '').replace('"', '').strip() and str(x).replace(',', '').replace('"', '').strip() != '-'  else None,
        },
        "dependent_tables": None
    },
    "crawler_news_latest": {
        "column_mapping": {
            'date': 'date',
            'title': 'title',
            'url': 'url',
            'content': 'content',
            'author': 'author',
            'publish_time': 'publish_time'
        },
        "drop_columns": None,
        "unique_columns": ['title', 'url'],
        "data_transforms": {
            'date': lambda x: pd.to_datetime(x,  format= '%d/%m/%Y', errors='coerce').date() if pd.notnull(x) else None,
            'publish_time': lambda x: pd.to_datetime(x, format='%H:%M %d/%m/%Y', errors='coerce') if pd.notnull(x) else None
        }
    },
    "crawler_news_stock":{
        "column_mapping" :{
            'symbol': 'stock',
            'date': 'date',
            'title': 'title',
            'url': 'url',
            'content': 'content',
            'author': 'author',
        },
        "drop_columns": ['publish_time'],
        "unique_columns": ['title', 'url'],
        "data_transforms": {
            'date': lambda x: pd.to_datetime(x,  format= '%d/%m/%Y', errors='coerce').date() if pd.notnull(x) else None,
        }
    }
}

def save_csv_to_postgres(csv_file, db_url, table_name, if_exists="replace"):
    """
    L∆∞u d·ªØ li·ªáu t·ª´ file CSV v√†o b·∫£ng PostgreSQL.

    Parameters:
    - csv_file (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV.
    - db_url (str): URL k·∫øt n·ªëi PostgreSQL.
    - table_name (str): T√™n b·∫£ng ƒë√≠ch.
    - if_exists (str): "replace" ho·∫∑c "append".
    """
    # Ki·ªÉm tra xem table_name c√≥ trong TABLE_MAPPINGS kh√¥ng
    if table_name not in TABLE_MAPPINGS:
        logging.error(f"Kh√¥ng t√¨m th·∫•y √°nh x·∫° cho b·∫£ng {table_name}. Vui l√≤ng th√™m v√†o TABLE_MAPPINGS.")
        return

    # L·∫•y th√¥ng tin √°nh x·∫°
    mapping_info = TABLE_MAPPINGS[table_name]
    column_mapping = mapping_info.get("column_mapping")
    drop_columns = mapping_info.get("drop_columns")
    unique_columns = mapping_info.get("unique_columns")
    data_transforms = mapping_info.get("data_transforms")
    dependent_tables = mapping_info.get("dependent_tables")

    # ƒê·ªçc file CSV
    try:
        df = pd.read_csv(csv_file)
        logging.info(f"ƒê·ªçc {len(df)} d√≤ng t·ª´ CSV th√†nh c√¥ng.")
    except Exception as e:
        logging.error(f"Kh√¥ng th·ªÉ ƒë·ªçc CSV file {csv_file}: {str(e)}")
        return
    
    logging.info(f"C√°c c·ªôt trong DataFrame: {df.columns.tolist()}")
    logging.info(f"D√≤ng ƒë·∫ßu ti√™n: {df.iloc[0].to_dict()}")

    # √Ånh x·∫° c·ªôt
    if column_mapping:
        df = df.rename(columns=column_mapping)
        logging.info(f"C√°c c·ªôt sau khi √°nh x·∫°: {df.columns.tolist()}")

    # X√≥a c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt
    if drop_columns:
        df = df.drop(columns=drop_columns, errors='ignore')

    # X·ª≠ l√Ω d·ªØ li·ªáu t√πy ch·ªânh
    try:
        if data_transforms:
            for column, transform in data_transforms.items():
                if column in df.columns:
                    try:
                        df[column] = df[column].apply(transform)
                        logging.info(f"ƒê√£ x·ª≠ l√Ω c·ªôt {column}")
                    except Exception as e:
                        logging.error(f"L·ªói khi x·ª≠ l√Ω c·ªôt {column}: {str(e)}")
                        return False
    except Exception as e:
        logging.error(f"L·ªói khi x·ª≠ l√Ω d·ªØ li·ªáu: {str(e)}")
        return False

    # Lo·∫°i b·ªè tr√πng l·∫∑p
    if unique_columns:
        df = df.drop_duplicates(subset=unique_columns)
        logging.info(f"S·ªë d√≤ng sau khi lo·∫°i tr√πng: {len(df)}")

    # K·∫øt n·ªëi t·ªõi PostgreSQL
    try:
        engine = create_engine(db_url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)
        logging.info("ƒê√£ k·∫øt n·ªëi ƒë·∫øn DB")
    except Exception as e:
        logging.error(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi c∆° s·ªü d·ªØ li·ªáu PostgreSQL: {str(e)}")
        return


    # X√≥a d·ªØ li·ªáu c≈© n·∫øu if_exists="replace"

    if if_exists == "replace":
        try:
            with engine.connect() as conn:
                # X√≥a d·ªØ li·ªáu trong c√°c b·∫£ng con (n·∫øu c√≥)
                if dependent_tables:
                    for dep_table in dependent_tables:
                        conn.execute(text(f"TRUNCATE TABLE {dep_table} CASCADE"))
                        logging.info(f"ƒê√£ x√≥a d·ªØ li·ªáu trong b·∫£ng con {dep_table}.")

                # X√≥a d·ªØ li·ªáu trong b·∫£ng ch√≠nh
                conn.execute(text(f"TRUNCATE TABLE {table_name} CASCADE"))
                conn.commit()
                logging.info(f"ƒê√£ x√≥a d·ªØ li·ªáu c≈© trong b·∫£ng {table_name}.")
        except Exception as e:
            logging.error(f"Kh√¥ng th·ªÉ TRUNCATE b·∫£ng {table_name}: {str(e)}")
            return


    """

    # X√≥a d·ªØ li·ªáu c≈© n·∫øu if_exists="replace"
    if if_exists == "replace":
        try:
            with engine.begin() as conn:  # üîß S·ª¨A
                # X√≥a d·ªØ li·ªáu trong c√°c b·∫£ng con (n·∫øu c√≥)
                if dependent_tables:
                    for dep_table in dependent_tables:
                        conn.execute(text(f"TRUNCATE TABLE {dep_table} CASCADE"))
                        logging.info(f"ƒê√£ x√≥a d·ªØ li·ªáu trong b·∫£ng con {dep_table}.")

                # X√≥a d·ªØ li·ªáu trong b·∫£ng ch√≠nh
                conn.execute(text(f"TRUNCATE TABLE {table_name} CASCADE"))
                logging.info(f"ƒê√£ x√≥a d·ªØ li·ªáu c≈© trong b·∫£ng {table_name}.")
        except Exception as e:
            logging.error(f"Kh√¥ng th·ªÉ TRUNCATE b·∫£ng {table_name}: {str(e)}")
            return
    """

    # Ghi d·ªØ li·ªáu v√†o b·∫£ng
    try:
        df.to_sql(
            table_name,
            engine,
            index=False,
            if_exists="append",
            method="multi",
            chunksize=1000
        )
        logging.info(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu {len(df)} d√≤ng v√†o b·∫£ng {table_name}.")

        # X√≥a file CSV sau khi l∆∞u th√†nh c√¥ng
        if os.path.exists(csv_file):
            os.remove(csv_file)
            logging.info(f"ƒê√£ x√≥a file CSV: {csv_file}")
            logging.info("================================================================")
        else:
            logging.warning(f"Kh√¥ng t√¨m th·∫•y file CSV ƒë·ªÉ x√≥a: {csv_file}")

    except Exception as e:
        logging.error(f"Kh√¥ng th·ªÉ l∆∞u d·ªØ li·ªáu v√†o b·∫£ng {table_name}: {str(e)}")
        raise    


